__author__ = 'quentin'

from pyrem import univariate
import unittest
import numpy as np



white_noise = np.array([-0.309,-0.988,-0.003,-0.686,0.562,-1.57,0.512,0.516,-0.867,1.413,0.073,-1.137,-0.916,0.859,-1.432,1.16,0.628,0.134,1.702,1.125,-0.947,0.076,-0.615,1.321,-1.048,0.598,-0.202,-0.113,-0.33,-1.342,-1.934,0.283,0.798,-1.307,0.565,-1.436,0.134,0.71,-0.362,-0.596,-0.147,1.612,0.619,-0.501,-1.042,-0.411,0,-0.93,-0.327,-1.126,1.146,-1.725,-1.696,-0.244,-0.371,0.945,-0.657,0.849,-0.532,1.356,-0.128,1.041,-0.946,1.096,2.066,1.14,-1.542,-1.795,-1.173,1.088,0.883,-0.159,1.127,0.305,1.797,0.416,-0.195,1.776,0.669,-0.926,0.142,-0.149,1.461,0.042,-0.678,0.961,0.413,-0.058,0.313,1.035,0.327,-0.095,1.118,-1.256,0.95,2.087,-0.24,-0.806,-0.952,0.122,
-1.574,-1.635,-0.506,-0.536,1.348,-0.938,-0.156,-0.283,-0.868,1.753,1.816,0.057,0.116,-1.227,0.028,-1.133,-0.051,-0.396,0.977,-0.585,0.897,-1.324,-0.697,0.997,1.033,-0.128,0.411,-0.28,0.832,1.432,1.035,1.077,0.583,1.014,-0.44,-1.435,-0.597,-1.05,-0.229,-0.767,0.614,-1.075,0.833,1.031,1.294,-0.158,1.33,1.303,-0.086,-2.471,0.187,1.076,-0.276,1.099,0.628,-0.999,0.12,-0.119,0.795,0.926,-0.594,-0.966,-0.765,-0.712,0.537,1.95,-0.697,0.784,-0.166,0.086,1.69,0.35,-1.206,0.023,0.226,0.736,0.481,-0.312,-0.159,1.938,-0.011,-0.822,-0.045,0.105,-1.037,-0.778,0.034,1.747,1.036,-0.396,-0.15,1.068,-1.32,-0.534,-1.289,0.933,0.322,0.745,-0.413,0.975,
0.887,0.587,-0.098,0.722,1.184,-0.466,-1.447,0.1,-1.56,-0.07,-0.631,0.635,1.156,-1.104,0.099,1.571,0.858,-0.351,-0.934,-1.033,1.535,0.953,-0.505,-1.023,-0.256,-1.446,0.253,0.375,-0.098,-1.816,-0.967,-0.177,-1.669,-0.275,-0.509,-0.248,-0.102,-0.875,1.043,-0.097,1.151,0.381,1.125,-2.47,1.555,-0.596,-0.322,-0.289,-0.975,-1.249,1.114,0.916,0.829,1.15,1.554,0.505,-0.573,1.184,1.181,-1.675,1.565,-3.171,-0.002,-0.244,0.072,0.34,1.799,0.285,-1.055,1.668,-1.45,-0.36,2.547,1.299,0.68,0.39,0.15,-2.882,-0.964,0.344,-1.596,0.608,1.776,-0.82,1.418,0.458,1.589,-0.452,-2.712,-2.633,0.273,0.667,1.06,0.456,0.257,-0.075,0.336,-1.186,0.253,1.239,
-0.736,-0.224,-0.895,-1.086,0.157,-0.422,0.274,1.505,0.485,0.127,0.284,0.647,-1.424,-0.746,0.022,0.396,-0.661,-0.338,-0.399,-1.931,-1.173,-1.639,0.069,0.147,0.08,0.051,0.186,-0.264,-0.22,-0.705,-1.526,0.052,0.952,-0.086,-0.145,-1.497,-1.946,-1.331,1.197,-0.423,0.104,-0.697,1.271,-0.568,-1.302,2.833,-0.471,0.624,-0.998,0.839,0.276,0.424,0.918,0.876,0.973,-1.944,0.508,-1.007,-0.144,-1.01,-0.587,-0.873,1.336,-0.213,-1.174,-0.222,1.423,3.283,-0.28,0.969,-0.026,-0.399,1.023,1.229,-0.648,-1.417,0.848,-1.637,0.066,-0.662,-0.111,-0.488,0.168,-0.252,-0.634,-1.488,0.715,1.177,0.578,-1.296,-0.898,0.021,0.116,1.861,-0.658,0.061,-0.477,0.58,-0.073,-0.585,
0.671,0.425,-0.171,0.106,0.417,-0.792,-0.082,0.24,0.248,-0.462,-0.666,-3.12,-1.512,-0.52,1.559,-0.485,0.089,0.28,-2.395,0.24,-1.373,-0.624,-1.94,-0.369,0.506,-0.966,-1.937,0.405,-0.482,-0.234,-0.26,-0.376,-0.683,-1.09,0.465,-0.686,-0.925,0.182,0.407,-0.539,0.857,-0.759,-0.666,1.807,0.156,1.42,0.513,-0.469,1.152,-0.713,-0.691,-0.883,-0.937,-0.807,-1.825,0.709,-0.423,0.34,-0.113,2.638,0.689,-0.123,0.128,0.656,-0.27,1.582,0.178,1.21,2.55,0.918,0.089,-1.546,0.842,0.131,0.831,0.834,0.108,1.079,0.007,1.726,1.247,0.719,-1.791,-1.692,0.766,-0.567,-2.404,-0.077,-1.811,-0.411,-0.836,1.005,1.941,0.138,-0.6,-0.528,-0.373,-0.024,0.892,0.607,
-0.695,-0.65,-1.362,-2.62,0.291,0.672,-0.987,-0.39,-0.779,0.752,0.068,-0.905,0.433,0.204,1.596,0.204,-0.775,-0.394,0.809,1.807,-0.104,-1.879,1.935,-1.495,0.745,-0.769,0.298,0.268,0.754,0.414,0.576,0.752,1.239,1.098,-0.507,0.206,-1.154,-0.905,-0.249,-1.153,-1.082,-2.15,1.188,0.73,0.549,-0.943,-2.803,-1.285,0.416,-0.732,-0.457,-0.023,0.511,-0.888,0.662,-1.9,1.878,0.367,0.261,-0.202,-0.375,0.709,-0.822,-0.41,-1.26,-0.196,-1.053,2.141,-0.024,-0.165,1.002,-0.169,0.03,-0.111,1.337,-1.375,1.148,-1.215,-0.06,1.587,-0.583,-0.444,-1.374,-0.45,-0.515,-1.334,1.184,-1.242,0.101,-0.253,-0.735,-0.654,-0.036,1.04,-0.039,1.355,-0.893,1.341,-1.147,1.269,
-1.826,0.171,-1.307,2.469,-1.747,0.869,-1.04,-0.315,-1.662,1.352,-0.177,-0.929,0.949,-0.497,0.58,1.264,0.203,0.382,1.608,-0.013,0.772,0.077,-1.633,-0.487,2.095,0.362,-0.093,-1.449,-1.031,0.55,-1.043,1.131,-0.698,0.202,1.017,-1.436,-1.695,2.303,2.05,-0.144,-1.023,0.698,0.294,-0.329,0.986,-0.095,-0.014,0.636,1.406,-0.465,0.038,-2.09,0.6,1.174,1.032,-0.41,0.232,0.541,0.514,1.994,0.603,-0.123,-0.989,-0.274,-0.359,-0.609,-0.6,1.408,0.521,-1.528,0.178,0.688,-0.497,-0.137,-1.553,-0.027,-0.154,-0.315,-0.437,-0.831,0.859,-1.751,0.593,0.369,0.935,-2.294,-2.667,0.073,0.924,-1.137,0.17,-0.211,0.062,0.033,1.387,-1.263,0.55,0.439,-0.841,0.65,
-0.197,-0.164,0.058,0.719,-0.313,-0.921,1.264,1.092,-0.455,-0.711,-1.278,-0.287,-0.532,0.037,1.3,0.692,0.059,1.144,-0.254,-0.623,0.845,-1.295,-1.28,0.885,1.301,0.64,-0.993,-1.541,0.279,1.176,0.446,0.781,0.51,-0.969,3.066,0.744,0.135,0.72,-0.544,1.627,-0.239,0.652,-0.301,-0.552,-0.478,0.351,0.909,-2.657,0.513,1.66,-0.133,0.212,0.765,0.189,0.666,-0.771,0.598,0.716,0.049,0.698,-1.466,0.304,-0.474,0.78,0.539,-0.87,-0.392,0.116,-1.139,-1.334,0.436,-0.282,0.158,-0.87,1.876,-3.213,-0.145,0.258,0.572,0.481,0.251,-0.308,-0.952,1.334,1.139,0.99,1.159,-0.997,0.107,-0.887,-3.066,-0.348,-0.145,-0.416,-0.107,1.104,1.974,0.607,-0.583,-0.203,
1.951,1.02,1.392,-0.951,-1.181,-0.549,-0.172,0.023,0.676,-0.405,0.728,-0.032,0.476,-1.502,0.638,-1.381,-0.318,-0.444,0.902,0.028,-0.554,-0.454,-1.314,-0.162,-0.841,1.422,0.152,-0.69,-0.147,0.41,0.494,-0.387,1.423,-1.652,-1.411,0.031,-0.056,0.402,1.178,-1.499,-0.722,-0.529,-1.302,1.217,1.663,-0.983,-0.846,0.848,0.037,0.623,0.242,-0.2,0.218,-0.184,0.602,-0.344,0.061,0.649,0.014,0.227,1.287,-0.114,-0.833,0.555,1.372,0.055,0.518,0.327,0.497,-0.604,-0.738,0.637,0.382,0.886,1.26,0.516,0.816,-1.147,0.225,1.348,-1.603,-1.846,-0.134,-0.513,0.164,-0.237,0.102,0.231,1.018,0.066,1.255,-0.497,-1.602,-0.96,-0.314,0.489,-0.428,-0.592,1.774,0.985,
-0.598,0.193,0.505,-0.887,0.673,-0.815,0.498,0.545,-0.269,1.511,-0.081,0.501,1.567,1.537,0.767,-0.366,-0.047,-0.535,0.912,0.631,-0.429,-1.777,0.102,-0.275,0.102,0.672,-0.598,0.384,-0.125,0.283,-0.826,-0.996,4.794,-1.839,-0.237,0.764,0.876,0.158,-0.961,0.96,-0.243,-0.787,-0.818,0.049,-0.478,-1.873,-0.079,1.515,0.572,-0.168,0.308,-0.296,-0.551,0.278,0.39,-0.822,-0.572,0.439,0.765,0.591,1.445,-0.469,-2.242,-0.942,1.914,0.833,-1.062,0.574,-1.275,-0.555,1.083,-0.151,0.745,1.723,-0.364,-0.788,-0.658,-1.274,0.628,-1.105,1.639,-0.214,0.357,1.922,0.618,-2.297,0.192,0.24,1.315,1.249,-0.67,1.294,0.516,0.312,0.919,-0.124,2.3,1.442,0.473,0.832])

class TestFeatures(unittest.TestCase):
	
    def test_pfd(self):
        ans = univariate.pfd(white_noise)
        ref =  1.0359881151548751
        self.assertAlmostEqual(ans, ref)
    def test_svd_entropy(self):
        ans = univariate.svd_entropy(white_noise,10,10)
        ref=3.3207292181222909
        self.assertAlmostEqual(ref, ans)

    def test_ap_entropy(self):
        ref =  0.39260453872556883
        ans = univariate.ap_entropy(white_noise, 2, 1.5)

        self.assertAlmostEqual(ref, ans)
    def test_samp_entropy(self):
        ref = 0.33704162039729635
        ans = univariate.samp_entropy(white_noise, 2, 1.5, 1,False)
        self.assertAlmostEqual(ref, ans)

        ref = 0.13338050823596964
        ans = univariate.samp_entropy(white_noise, 3, 1.5, 2, False)
        
        self.assertAlmostEqual(ref, ans)
#~ 
        # TODO
    #~ def test_fisher_information(self):
        #~ ref = 0
        #~ ans = univariate.dfa(white_noise)
#~ 
        #~ self.assertAlmostEqual(ref, ans)
        # self.assertAlmostEqual(ref, ans)

    def test_fisher_information(self):
        ref = 0.0002986254447524082 
        ans = univariate.fisher_info(white_noise,10,10)

        self.assertAlmostEqual(ref, ans)

    def test_hfd(self):

        ans = univariate.hfd(white_noise[:100], 10)
        ref =  2.0703560530609164
        self.assertAlmostEqual(ref, ans)
        ans = univariate.hfd(np.cumsum(white_noise[:10000]), 20)
        ref = 1.5369261439430244
        self.assertAlmostEqual(ref, ans)



    def test_hjorth(self):
        ans_activity, ans_morbidity, ans_complexity= univariate.hjorth(white_noise)
        
        ref_complexity = 1.2311281218979759 
        ref_morbidity = 1.4009964711379468
        ref_activity = 1.0152445860000012
        
        self.assertAlmostEqual(ref_complexity, ans_complexity)
        self.assertAlmostEqual(ref_morbidity, ans_morbidity)
        self.assertAlmostEqual(ref_activity, ans_activity)
